# Grid-Based Pathfinding: A* and Beam Search with A*-Style Pruning

This document details the pathfinding algorithms implemented for navigating a grid, potentially of very large conceptual dimensions. The core algorithms discussed are A* Search and Beam Search, with Beam Search utilizing an A*-style evaluation function for its pruning step.

## Table of Contents
1.  [Core Concepts](#core-concepts)
    *   [Grid Representation](#grid-representation)
    *   [Nodes](#nodes)
    *   [Cost Functions: `g(n)`](#cost-function-gn)
    *   [Heuristic Function: `h(n)`](#heuristic-function-hn)
    *   [Evaluation Function: `f(n)`](#evaluation-function-fn)
2.  [A* Search Algorithm](#a-search-algorithm)
    *   [Mathematical Foundation](#mathematical-foundation-a)
    *   [Algorithm Mechanics](#algorithm-mechanics-a)
    *   [Data Structures](#data-structures-a)
    *   [Optimality and Completeness](#optimality-and-completeness-a)
    *   [Time and Space Complexity (A*)](#time-and-space-complexity-a)
3.  [Beam Search Algorithm](#beam-search-algorithm)
    *   [Mathematical Foundation & A*-Style Pruning](#mathematical-foundation--a-style-pruning-beam)
    *   [Algorithm Mechanics](#algorithm-mechanics-beam)
    *   [Beam Width (W)](#beam-width-w)
    *   [Trade-offs: Optimality vs. Efficiency](#trade-offs-optimality-vs-efficiency-beam)
    *   [Time and Space Complexity (Beam)](#time-and-space-complexity-beam)
4.  [Procedural Obstacle Generation](#procedural-obstacle-generation)

## 1. Core Concepts

### Grid Representation
The pathfinding environment is a 2D grid. Movement is typically restricted to adjacent cells (up, down, left, right), often referred to as 4-directional movement. For very large grids (e.g., 500,000 x 500,000), an **implicit grid** representation is used. Instead of storing the entire grid in memory, the state of a cell (whether it's an obstacle or traversable) is determined procedurally on-demand.

### Nodes
A **node** (`n`) in the search space represents a specific cell `(row, col)` on the grid. Each node typically stores:
*   Its position.
*   A reference to its parent node (the node from which it was reached).
*   The cost `g(n)`: The actual cost of the path from the start node to node `n`.
*   The heuristic cost `h(n)`: An estimated cost of the cheapest path from node `n` to the goal node.
*   The evaluation cost `f(n)`: The sum of `g(n)` and `h(n)`.

### Cost Function: `g(n)`
The function `g(n)` represents the exact accumulated cost to reach node `n` from the starting node. In a grid where each step has a uniform cost (e.g., cost of 1 to move to an adjacent cell), `g(n)` is simply the length of the path from the start to `n`.
If `n'` is a successor of `n`:
`g(n') = g(n) + cost(n, n')`
where `cost(n, n')` is the cost of moving from `n` to `n'`.

### Heuristic Function: `h(n)`
The heuristic function `h(n)` estimates the cost of the cheapest path from node `n` to the goal node. The choice of heuristic is crucial for the performance and optimality of A*.
*   **Admissibility:** A heuristic is **admissible** if it *never overestimates* the actual cost to reach the goal. That is, `h(n) ≤ actual_cost(n, goal)` for all nodes `n`. Admissibility is required for A* to guarantee an optimal solution.
*   **Consistency (or Monotonicity):** A heuristic is **consistent** if, for every node `n` and every successor `n'` of `n` generated by any action `a`:
    `h(n) ≤ cost(n, n') + h(n')`
    A consistent heuristic is always admissible.

For grid pathfinding with 4-directional movement and uniform step costs, the **Manhattan Distance** is a common admissible and consistent heuristic:
`h((x₁, y₁), (x₂, y₂)) = |x₁ - x₂| + |y₁ - y₂|`
where `(x₁, y₁)` are the coordinates of the current node and `(x₂, y₂)` are the coordinates of the goal node.

### Evaluation Function: `f(n)`
Both A* and the A*-style pruning in Beam Search use an evaluation function `f(n)` to estimate the total cost of a solution path going through node `n`:
**`f(n) = g(n) + h(n)`**

This function combines the known cost to reach `n` (`g(n)`) with an estimate of the cost from `n` to the goal (`h(n)`). The algorithms prioritize exploring nodes with lower `f(n)` values.

---

## 2. A* Search Algorithm

### Mathematical Foundation (A*)
A* is an informed search algorithm that aims to find the least-cost path from a given initial node to one goal node. It uses the evaluation function `f(n) = g(n) + h(n)` to guide its search. A* maintains a priority queue of nodes to visit (the "Open Set"), ordered by their `f(n)` values.

### Algorithm Mechanics (A*)
1.  **Initialization:**
    *   Create an **Open Set** (priority queue) and add the start node to it. `g(start) = 0`, `h(start)` is calculated, `f(start) = h(start)`.
    *   Create a **Closed Set** (hash set or similar) to store nodes that have already been evaluated. Initially empty.
    *   Maintain a data structure (e.g., a dictionary `g_costs`) to store the lowest `g(n)` value found so far to reach node `n`.

2.  **Loop:**
    *   While the Open Set is not empty:
        a.  Remove the node `n` from the Open Set that has the lowest `f(n)` value.
        b.  If `n` is the goal node, reconstruct the path by backtracking from `n` to the start using parent pointers. Terminate successfully.
        c.  Add `n` to the Closed Set.
        d.  For each **neighbor** `m` of `n` (valid, traversable, and within grid bounds):
            i.  If `m` is in the Closed Set, ignore it (as a shorter path to `m` would have already been found if the heuristic is consistent).
            ii. Calculate `tentative_g_score = g(n) + cost(n, m)`.
            iii. If `tentative_g_score < g_costs.get(m, infinity)`:
                *   This path to `m` is better than any previously found.
                *   Set `parent(m) = n`.
                *   Set `g(m) = tentative_g_score`.
                *   Calculate `h(m)` using the heuristic function.
                *   Calculate `f(m) = g(m) + h(m)`.
                *   If `m` is not in the Open Set, add it. Otherwise, update its priority in the Open Set (if its `f(m)` value has decreased). Store `g_costs[m] = g(m)`.

3.  **Termination:**
    *   If the Open Set becomes empty and the goal has not been reached, no path exists.

### Data Structures (A*)
*   **Open Set:** Typically implemented as a min-priority queue (e.g., using a binary heap).
    *   Insertion: `O(log V)`
    *   Extraction of min: `O(log V)`
    *   Decrease-key (update): Can be `O(log V)` or `O(V)` if not directly supported and requires remove/re-add. Python's `heapq` doesn't directly support decrease-key efficiently; one common workaround is to add the updated node again and rely on processing the one with the better score first (possibly leading to duplicate entries with different scores in the heap, managed by `g_costs`).
*   **Closed Set:** Typically a hash set.
    *   Insertion: `O(1)` on average
    *   Lookup: `O(1)` on average
*   **`g_costs` Dictionary:** Hash map.
    *   Insertion/Update/Lookup: `O(1)` on average

### Optimality and Completeness (A*)
*   **Completeness:** A* is complete (i.e., it will always find a solution if one exists) provided the search space is finite (or step costs are bounded above zero, preventing infinite paths of finite cost).
*   **Optimality:** A* is guaranteed to find the optimal (least-cost) path if the heuristic function `h(n)` is **admissible**. If `h(n)` is also **consistent**, A* is optimally efficient, meaning no other optimal algorithm that uses the same heuristic will expand fewer nodes (on average, for non-pathological cases).

### Time and Space Complexity (A*)
Let `V` be the number of nodes in the graph (or nodes visited) and `E` be the number of edges (or transitions explored). In a grid, `E` is proportional to `V` (e.g., `E ≈ 4V`).

*   **Time Complexity:**
    *   In the worst case, A* might explore a large portion of the state space. Each node is added to the Open Set at most once (if `h` is consistent).
    *   For each node popped from the Open Set, its neighbors are considered.
    *   The dominant operations are insertions and deletions from the priority queue (Open Set).
    *   If using a binary heap for the priority queue: `O(E log V)` or `O(V log V)` since `E` can be `O(V)`.
    *   If a Fibonacci heap is used (supporting efficient decrease-key), the amortized complexity can be `O(E + V log V)`.
    *   The actual number of nodes visited depends heavily on the quality of the heuristic. A perfect heuristic `h(n) = actual_cost(n, goal)` would lead A* directly to the goal. A poor heuristic (e.g., `h(n) = 0`, making A* behave like Dijkstra's) could explore many nodes.

*   **Space Complexity:**
    *   A* needs to store the Open Set and the Closed Set. In the worst case, all nodes might be stored.
    *   Therefore, space complexity is typically `O(V)`.
    *   This can be a significant limitation in very large state spaces.

---

## 3. Beam Search Algorithm

Beam Search is a heuristic search algorithm that explores a graph by expanding the most promising nodes in a limited set. It is a variation of breadth-first search that limits its search to a fixed number of states at each level – the **beam width (`W`)**.

### Mathematical Foundation & A*-Style Pruning (Beam)
The variant implemented uses an **A*-style evaluation function `f(n) = g(n) + h(n)`** for its pruning step. This means that at each level of expansion, successor nodes are evaluated based on their `f`-scores, and only the best `W` nodes are kept for further expansion.
This differs from standard A* because:
*   It does not maintain a global Open Set of all discovered nodes.
*   It prunes paths aggressively at each level, meaning it might discard a node that could have led to an optimal solution if that node's `f`-score was not within the top `W` at that particular step.

### Algorithm Mechanics (Beam)
1.  **Initialization:**
    *   Create the initial **beam** (a list or array) containing only the start node. `g(start) = 0`, `h(start)` is calculated.
    *   Maintain a data structure `visited_g_costs` to store the best `g(n)` value found so far by the beam search to reach node `n`. This helps avoid cycles and redundant expansions of states already reached more cheaply.

2.  **Loop (Iterate Level by Level):**
    *   For a predefined maximum number of iterations/depth (`D_max`) or until the beam is empty:
        a.  Create an empty list of `candidates` for the next beam.
        b.  For each node `n` in the `current_beam` (size at most `W`):
            i.  If `n` is the goal node, reconstruct the path and terminate successfully.
            ii. For each **neighbor** `m` of `n` (at most `b` neighbors, where `b` is the branching factor, e.g., 4 for a grid):
                1.  Calculate `tentative_g_score = g(n) + cost(n, m)`.
                2.  If `tentative_g_score >= visited_g_costs.get(m, infinity)`, ignore `m`.
                3.  Update `visited_g_costs[m] = tentative_g_score`.
                4.  Set `parent(m) = n`, `g(m) = tentative_g_score`.
                5.  Calculate `h(m)` and `f(m) = g(m) + h(m)`.
                6.  Add `m` to the `candidates` list. The `candidates` list can grow up to `W * b` in size.
        c.  If `candidates` list is empty, terminate.
        d.  **Pruning Step:** Sort all `candidates` by their `f(m)` values. This takes `O( (W*b) log (W*b) )`.
        e.  Select the top `W` candidates to form the `next_beam`.
        f.  Set `current_beam = next_beam`.

3.  **Termination:** Goal found, beam empty, or max depth reached.

### Beam Width (W)
*   The **beam width `W`** is a crucial parameter.
    *   If `W = 1`, Beam Search becomes a greedy best-first search.
    *   A larger `W` explores more, increasing chances of better solutions but also cost.
    *   A smaller `W` is faster but more likely to prune good paths.

### Trade-offs: Optimality vs. Efficiency (Beam)
*   **Not Optimal:** Generally not optimal.
*   **Not Complete (Strictly):** Might fail to find a solution even if one exists if the path is pruned.
*   **Efficiency:** Often more memory-efficient and can be faster than A* for very large search spaces.

### Time and Space Complexity (Beam)
Let `W` be the beam width.
Let `b` be the branching factor (maximum number of successors of a node, e.g., 4 for a grid).
Let `D` be the depth of the solution (or maximum depth searched).

*   **Time Complexity:**
    *   At each of the `D` levels:
        *   Generating successors for all `W` nodes in the current beam: `O(W * b)` operations.
        *   Sorting the candidates: The number of candidates can be up to `W * b`. Sorting takes `O((W*b) log (W*b))`.
    *   Therefore, the total time complexity is roughly `O(D * W * b * log(W*b))`.
    *   The `visited_g_costs` lookups and insertions are `O(1)` on average.

*   **Space Complexity:**
    *   The primary space is used to store the `current_beam` (size `W`) and the `candidates` list (size up to `W*b` before pruning).
    *   The `visited_g_costs` map can grow. In the worst case, if the beam explores many unique states before reaching the goal or max depth, it could store up to `D * W` entries if many states are distinct across levels. However, if the search is more focused, it might be smaller. If paths frequently reconverge, `visited_g_costs` might store fewer unique states than `D*W`.
    *   So, space is roughly `O(W*b)` for temporary candidate storage plus the size of `visited_g_costs`. If `visited_g_costs` stores most nodes visited, it could be `O(min(Total States, D*W))`. For practical purposes with a fixed `W`, it's often considered `O(W*b + Size_of_Visited_Set)`. Compared to A*'s `O(V)`, if `W` is small, Beam Search is much more memory efficient.

---

## 4. Procedural Obstacle Generation

For extremely large grids where explicit storage is infeasible, obstacles are generated procedurally. This involves a deterministic function:
`is_obstacle(position, scenario_parameters) -> boolean`

This function takes a cell's `position (row, col)` and `scenario_parameters` (such as a unique `scenario_seed` and `obstacle_density`) and returns `true` if the cell is an obstacle, `false` otherwise.
The key aspects are:
*   **Determinism:** For the same position and scenario parameters, the function must always return the same result.
*   **Pseudo-Randomness:** Typically, a pseudo-random number generator (PRNG) is seeded based on the cell coordinates and the scenario seed. The generated random number is then compared against the `obstacle_density`.
    *   `random_value = PRNG(hash(row, col, scenario_seed))`
    *   `is_obstacle = random_value < obstacle_density`
*   **Start/Goal Exemption:** The start and goal positions are usually explicitly defined as non-obstacles.

This approach avoids storing the grid while providing a consistent and complex environment for the pathfinders. The quality of the PRNG and the hashing/mixing of coordinates and seed can affect the perceived "randomness" and distribution of obstacles. The cost of calling this function for each neighbor check adds to the overall runtime but is usually a constant factor per node expansion.